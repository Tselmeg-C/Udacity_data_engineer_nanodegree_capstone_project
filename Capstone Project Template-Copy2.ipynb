{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Data Engineering for Analysis on i94Immigration Data from US\n",
    "### Udacity Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "In this project, I worked with four datasets from different sources, designed a Star Schema for those data and prepared them ready for interested analysis on immigration to USA. \n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# imports and installs \n",
    "import pandas as pd\n",
    "import os\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession,Window\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data (see README)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\t# creating sparksession \n",
    "spark = SparkSession.builder.\\\n",
    "                    config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\"). \\\n",
    "                    enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.1 I94 Immigration Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1.1 Data Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load i94immigration data from local\n",
    "df_sas =spark.read.format('com.github.saurfang.sas.spark') \\\n",
    "            .load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "#write to parquet\n",
    "#df_sas.write.parquet(\"sas_data\")\n",
    "# read in parquet files\n",
    "df_sas=spark.read.parquet(\"sas_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cicid: double (nullable = true)\n",
      " |-- i94yr: double (nullable = true)\n",
      " |-- i94mon: double (nullable = true)\n",
      " |-- i94cit: double (nullable = true)\n",
      " |-- i94res: double (nullable = true)\n",
      " |-- i94port: string (nullable = true)\n",
      " |-- arrdate: double (nullable = true)\n",
      " |-- i94mode: double (nullable = true)\n",
      " |-- i94addr: string (nullable = true)\n",
      " |-- depdate: double (nullable = true)\n",
      " |-- i94bir: double (nullable = true)\n",
      " |-- i94visa: double (nullable = true)\n",
      " |-- count: double (nullable = true)\n",
      " |-- dtadfile: string (nullable = true)\n",
      " |-- visapost: string (nullable = true)\n",
      " |-- occup: string (nullable = true)\n",
      " |-- entdepa: string (nullable = true)\n",
      " |-- entdepd: string (nullable = true)\n",
      " |-- entdepu: string (nullable = true)\n",
      " |-- matflag: string (nullable = true)\n",
      " |-- biryear: double (nullable = true)\n",
      " |-- dtaddto: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- insnum: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- admnum: double (nullable = true)\n",
      " |-- fltno: string (nullable = true)\n",
      " |-- visatype: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explore data: check columns\n",
    "df_sas.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|    cicid| i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|occup|entdepa|entdepd|entdepu|matflag|biryear| dtaddto|gender|insnum|airline|        admnum|fltno|visatype|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "|5748517.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     CA|20582.0|  40.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1976.0|10292016|     F|  null|     QF|9.495387003E10|00011|      B1|\n",
      "|5748518.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     NV|20591.0|  32.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1984.0|10292016|     F|  null|     VA|9.495562283E10|00007|      B1|\n",
      "|5748519.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20582.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     M|  null|     DL|9.495640653E10|00040|      B1|\n",
      "|5748520.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  29.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1987.0|10292016|     F|  null|     DL|9.495645143E10|00040|      B1|\n",
      "|5748521.0|2016.0|   4.0| 245.0| 438.0|    LOS|20574.0|    1.0|     WA|20588.0|  28.0|    1.0|  1.0|20160430|     SYD| null|      G|      O|   null|      M| 1988.0|10292016|     M|  null|     DL|9.495638813E10|00040|      B1|\n",
      "+---------+------+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-----+-------+-------+-------+-------+-------+--------+------+------+-------+--------------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explore data: show first 5 rows\n",
    "df_sas.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-------+\n",
      "|airline|i94port|i94addr|\n",
      "+-------+-------+-------+\n",
      "|ZZ     |MIA    |NC     |\n",
      "|ZZ     |WAS    |NY     |\n",
      "|ZZ     |SEA    |WA     |\n",
      "|ZZ     |SFR    |DE     |\n",
      "|ZZ     |NYC    |NJ     |\n",
      "|ZZ     |ADW    |null   |\n",
      "|ZZ     |SFR    |NY     |\n",
      "|ZZ     |MIA    |FL     |\n",
      "|ZZ     |BRO    |TX     |\n",
      "|ZZ     |NYC    |CT     |\n",
      "|ZZ     |ATL    |NY     |\n",
      "|ZZ     |NEW    |HI     |\n",
      "|ZZ     |ADW    |MD     |\n",
      "|ZZ     |NYC    |NY     |\n",
      "|ZZ     |HHW    |HI     |\n",
      "|ZX     |WAS    |IN     |\n",
      "|ZX     |FTL    |MN     |\n",
      "|ZX     |TOR    |GA     |\n",
      "|ZX     |FTL    |OH     |\n",
      "|ZX     |HHW    |CT     |\n",
      "+-------+-------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explore data: check interested columns \n",
    "df_sas.select(col('airline'),col('i94port'),col('i94addr')).distinct().sort(df_sas.airline.desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3096313"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# explore data: count rows\n",
    "df_sas.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|cicid|i94yr|i94mon|i94cit|i94res|i94port|arrdate|i94mode|i94addr|depdate|i94bir|i94visa|count|dtadfile|visapost|  occup|entdepa|entdepd|entdepu|matflag|biryear|dtaddto|gender| insnum|airline|admnum|fltno|visatype|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "|    0|    0|     0|     0|     0|      0|      0|    239| 152592| 142457|   802|      0|    0|       1| 1881250|3088187|    238| 138429|3095921| 138429|    802|    477|414269|2982605|  83627|     0|19549|       0|\n",
      "+-----+-----+------+------+------+-------+-------+-------+-------+-------+------+-------+-----+--------+--------+-------+-------+-------+-------+-------+-------+-------+------+-------+-------+------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explore data: Get count of both null and missing values\n",
    "df_sas.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_sas.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.1.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clear_df_sas(read_format, input_data, output_data):\n",
    "    \"\"\"\n",
    "    load df_sas data and clean it and write to output_data\n",
    "    Params:\n",
    "        read_format  : format of the input data\n",
    "        input_data   : stored location of df_sas\n",
    "        output_data  : location of output data to be saved. \n",
    "    \"\"\"\n",
    "    # load i94immigration data from local\n",
    "    df_sas =spark.read.format(read_format) \\\n",
    "            .load(input_data)\n",
    "    \n",
    "    # transfer dates to spark datetype, format column names and datatypes, drop columns and duplicated rows\n",
    "    df_sas = df_sas.withColumn(\"data_base_sas\", to_date(lit(\"01/01/1960\"), \"MM/dd/yyyy\")) \\\n",
    "            .withColumn(\"arrival_date\", expr(\"date_add(data_base_sas, arrdate)\")) \\\n",
    "            .withColumn(\"departure_date\", expr(\"date_add(data_base_sas, depdate)\")) \\\n",
    "            .drop(\"data_base_sas\", \"arrdate\", \"depdate\") \\\n",
    "            .withColumn(\"cic_id\",col(\"cicid\").cast(IntegerType())).drop(\"cicid\") \\\n",
    "            .withColumn(\"arrive_year\",col('i94yr').cast(IntegerType())).drop(\"i94yr\") \\\n",
    "            .withColumn(\"arrive_month\",col('i94mon').cast(IntegerType())).drop(\"i94mon\") \\\n",
    "            .withColumn(\"citizen_country\",col('i94cit').cast(IntegerType())).drop(\"i94cit\") \\\n",
    "            .withColumn(\"resident_country\",col('i94res').cast(IntegerType())).drop(\"i94res\") \\\n",
    "            .withColumn(\"age\",col('i94bir').cast(IntegerType())).drop(\"i94bir\") \\\n",
    "            .withColumn(\"birth_year\",col('biryear').cast(IntegerType())).drop(\"biryear\") \\\n",
    "            .withColumn(\"visa_class\",col('i94visa').cast(IntegerType())).drop(\"i94visa\") \\\n",
    "            .withColumn(\"mode\",col('i94mode').cast(IntegerType())).drop(\"i94mode\") \\\n",
    "            .withColumn(\"allowed_date\", to_date(\"dtaddto\", \"MMddyyyy\")) \\\n",
    "            .withColumnRenamed(\"i94port\", \"port\") \\\n",
    "            .withColumnRenamed(\"i94addr\",\"arrive_state\") \\\n",
    "            .withColumnRenamed(\"visapost\",\"visa_issue_state\") \\\n",
    "            .withColumnRenamed(\"entdepa\",\"arrive_flag\") \\\n",
    "            .withColumnRenamed(\"entdepd\",\"departure_flag\") \\\n",
    "            .withColumnRenamed(\"matflag\",\"match_flag\") \\\n",
    "            .withColumnRenamed(\"entdepu\",\"update_flag\") \\\n",
    "            .withColumnRenamed(\"fltno\",\"flight_num\") \\\n",
    "            .withColumnRenamed(\"visatype\",\"visa_type\") \\\n",
    "            .withColumnRenamed(\"visapost\",\"visa_issue_state\") \\\n",
    "            .withColumnRenamed(\"occup\",\"occupation\") \\\n",
    "            .drop('count','dtadfile','insnum','admnum','dtaddto') \\\n",
    "            .distinct()\n",
    "    \n",
    "    # write the end data to parquet file in workspace\n",
    "    # remove directory if already exist\n",
    "    if os.path.exists(output_data + 'immigration'):\n",
    "        os.rmdir(output_data + 'immigration')\n",
    "    else:\n",
    "        df_sas.write.partitionBy('arrive_state').parquet('immigration')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# initiate the prams and call the clear_df_sas function to clean the df_sas data and write the end data to local\n",
    "df_sas_format    = 'com.github.saurfang.sas.spark'\n",
    "df_sas_read_from = '../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat'\n",
    "df_sas_save_to   = 'file:/home/workspace/'\n",
    "clear_df_sas(df_sas_format,df_sas_read_from,df_sas_save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- port: string (nullable = true)\n",
      " |-- visa_issue_state: string (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      " |-- arrive_flag: string (nullable = true)\n",
      " |-- departure_flag: string (nullable = true)\n",
      " |-- update_flag: string (nullable = true)\n",
      " |-- match_flag: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- airline: string (nullable = true)\n",
      " |-- flight_num: string (nullable = true)\n",
      " |-- visa_type: string (nullable = true)\n",
      " |-- arrival_date: date (nullable = true)\n",
      " |-- departure_date: date (nullable = true)\n",
      " |-- cic_id: integer (nullable = true)\n",
      " |-- arrive_year: integer (nullable = true)\n",
      " |-- arrive_month: integer (nullable = true)\n",
      " |-- citizen_country: integer (nullable = true)\n",
      " |-- resident_country: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- birth_year: integer (nullable = true)\n",
      " |-- visa_class: integer (nullable = true)\n",
      " |-- mode: integer (nullable = true)\n",
      " |-- allowed_date: date (nullable = true)\n",
      " |-- arrive_state: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in cleaned df_sas, check schema\n",
    "df_sas_clear = spark.read.parquet('immigration')\n",
    "df_sas_clear.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------------+----------+-----------+--------------+-----------+----------+------+-------+----------+---------+------------+--------------+-------+-----------+------------+---------------+----------------+---+----------+----------+----+------------+------------+\n",
      "|port|visa_issue_state|occupation|arrive_flag|departure_flag|update_flag|match_flag|gender|airline|flight_num|visa_type|arrival_date|departure_date| cic_id|arrive_year|arrive_month|citizen_country|resident_country|age|birth_year|visa_class|mode|allowed_date|arrive_state|\n",
      "+----+----------------+----------+-----------+--------------+-----------+----------+------+-------+----------+---------+------------+--------------+-------+-----------+------------+---------------+----------------+---+----------+----------+----+------------+------------+\n",
      "| AGA|            null|      null|          G|             O|       null|         M|     M|     UA|     00150|       WT|  2016-04-30|    2016-05-02|5750212|       2016|           4|            254|             209| 62|      1954|         2|   1|  2016-07-28|          GU|\n",
      "| AGA|            null|      null|          G|             O|       null|         M|     F|     UA|     00150|       WT|  2016-04-30|    2016-05-02|5750213|       2016|           4|            254|             209| 44|      1972|         2|   1|  2016-07-28|          GU|\n",
      "| AGA|            null|      null|          G|             O|       null|         M|     F|     UA|     00178|       WT|  2016-04-30|    2016-05-03|5750214|       2016|           4|            254|             209| 71|      1945|         2|   1|  2016-07-29|          GU|\n",
      "| AGA|            null|      null|          G|             O|       null|         M|     F|     UA|     00178|       WT|  2016-04-30|    2016-05-03|5750215|       2016|           4|            254|             209| 65|      1951|         2|   1|  2016-07-29|          GU|\n",
      "| AGA|            null|      null|          G|             O|       null|         M|     M|     UA|     00178|       WT|  2016-04-30|    2016-05-03|5750216|       2016|           4|            254|             209| 65|      1951|         2|   1|  2016-07-29|          GU|\n",
      "+----+----------------+----------+-----------+--------------+-----------+----------+------+-------+----------+---------+------------+--------------+-------+-----------+------------+---------------+----------------+---+----------+----------+----+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show first 5 rows of clean data\n",
    "df_sas_clear.show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.2 U.S. City Demographic Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.2.1 Data Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|            City|        State|Median Age|Male Population|Female Population|Total Population|Number of Veterans|Foreign-born|Average Household Size|State Code|                Race|Count|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "|   Silver Spring|     Maryland|      33.8|          40601|            41862|           82463|              1562|       30908|                   2.6|        MD|  Hispanic or Latino|25924|\n",
      "|          Quincy|Massachusetts|      41.0|          44129|            49500|           93629|              4147|       32935|                  2.39|        MA|               White|58723|\n",
      "|          Hoover|      Alabama|      38.5|          38040|            46799|           84839|              4819|        8229|                  2.58|        AL|               Asian| 4759|\n",
      "|Rancho Cucamonga|   California|      34.5|          88127|            87105|          175232|              5821|       33878|                  3.18|        CA|Black or African-...|24437|\n",
      "|          Newark|   New Jersey|      34.6|         138040|           143873|          281913|              5829|       86253|                  2.73|        NJ|               White|76402|\n",
      "+----------------+-------------+----------+---------------+-----------------+----------------+------------------+------------+----------------------+----------+--------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#load us-cities-demographics.csv\n",
    "df_demo = spark.read.csv(\"us-cities-demographics.csv\",header=True,sep=\";\")\n",
    "df_demo.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Median Age: string (nullable = true)\n",
      " |-- Male Population: string (nullable = true)\n",
      " |-- Female Population: string (nullable = true)\n",
      " |-- Total Population: string (nullable = true)\n",
      " |-- Number of Veterans: string (nullable = true)\n",
      " |-- Foreign-born: string (nullable = true)\n",
      " |-- Average Household Size: string (nullable = true)\n",
      " |-- State Code: string (nullable = true)\n",
      " |-- Race: string (nullable = true)\n",
      " |-- Count: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2891"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_demo.printSchema()\n",
    "df_demo.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------+----+-----+----------+---------------+-----------------+----------------+------------+-----------------------+------------------+\n",
      "|city|state|state_code|race|count|median_age|male_population|female_population|total_population|veterans_num|foreign_born_population|avg_household_size|\n",
      "+----+-----+----------+----+-----+----------+---------------+-----------------+----------------+------------+-----------------------+------------------+\n",
      "|   0|    0|         0|   0|    0|         0|              3|                3|               0|          13|                     13|                16|\n",
      "+----+-----+----------+----+-----+----------+---------------+-----------------+----------------+------------+-----------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Get count of both null and missing values\n",
    "df_demo.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_demo.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------+--------------------+-----+---------------+-----------------+------------+-----------------------+------------------+\n",
      "|        city|  state|                race|count|male_population|female_population|veterans_num|foreign_born_population|avg_household_size|\n",
      "+------------+-------+--------------------+-----+---------------+-----------------+------------+-----------------------+------------------+\n",
      "|The Villages|Florida|               White|72211|           null|             null|       15231|                   4034|              null|\n",
      "|The Villages|Florida|Black or African-...|  331|           null|             null|       15231|                   4034|              null|\n",
      "|The Villages|Florida|  Hispanic or Latino| 1066|           null|             null|       15231|                   4034|              null|\n",
      "+------------+-------+--------------------+-----+---------------+-----------------+------------+-----------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# show rows with null values in 'male_population' column\n",
    "df_demo.select('city','state','race','count','male_population','female_population','veterans_num','foreign_born_population','avg_household_size') \\\n",
    "    .where(col('male_population').isNull()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+\n",
      "|race                             |\n",
      "+---------------------------------+\n",
      "|Black or African-American        |\n",
      "|Hispanic or Latino               |\n",
      "|White                            |\n",
      "|Asian                            |\n",
      "|American Indian and Alaska Native|\n",
      "+---------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check distinct values in 'race' column\n",
    "df_demo.select('race').distinct().show(truncate = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.2.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_df_demography(delimiter, input_data, output_data):\n",
    "    \"\"\"\n",
    "    cleaning df_demo data\n",
    "    prams:\n",
    "        delimiter  : delimiter of csv file\n",
    "        input_data : location of the source data\n",
    "        output_data: location where to save the clean data\n",
    "\n",
    "    \"\"\"\n",
    "    df_demo = spark.read.csv(input_data,header=True,sep = delimiter)\n",
    "\n",
    "    \n",
    "    # change datatypes, format column names and delete duplicates\n",
    "    df_demo = df_demo.withColumn(\"median_age\",col(\"Median Age\").cast(FloatType())).drop(\"Median Age\") \\\n",
    "        .withColumn(\"male_population\",col(\"Male Population\").cast(IntegerType())).drop(\"Male Population\") \\\n",
    "        .withColumn(\"female_population\",col(\"Female Population\").cast(IntegerType())).drop(\"Female Population\") \\\n",
    "        .withColumn(\"total_population\",col(\"Total Population\").cast(IntegerType())).drop(\"Total Population\") \\\n",
    "        .withColumn(\"veterans_num\",col(\"Number of Veterans\").cast(IntegerType())).drop(\"Number of Veterans\") \\\n",
    "        .withColumn(\"foreign_born_population\",col(\"Foreign-born\").cast(IntegerType())).drop(\"Foreign-born\") \\\n",
    "        .withColumn(\"avg_household_size\",col(\"Average Household Size\").cast(FloatType())).drop(\"Average Household Size\") \\\n",
    "        .withColumn(\"count\",col(\"Count\").cast(IntegerType())) \\\n",
    "        .withColumnRenamed(\"City\", \"city\") \\\n",
    "        .withColumnRenamed(\"State\", \"state\") \\\n",
    "        .withColumnRenamed(\"State Code\", \"state_code\") \\\n",
    "        .withColumnRenamed(\"Race\", \"race\") \\\n",
    "        .distinct()\n",
    "    \n",
    "    # pivot table to make each race population into seperate columns, change column names\n",
    "    df_demo = df_demo.groupBy(col(\"city\"),col(\"state\"),col(\"median_age\") \\\n",
    "                        ,col(\"male_population\"),col(\"female_population\") \\\n",
    "                        ,col(\"total_population\"),col(\"veterans_num\") \\\n",
    "                        ,col(\"foreign_born_population\"),col(\"avg_household_size\") \\\n",
    "                        ,col(\"state_code\")) \\\n",
    "                    .pivot(\"race\").agg(sum(\"count\").cast(\"integer\")) \\\n",
    "                    .fillna({\"American Indian and Alaska Native\": 0,\n",
    "                     \"Asian\": 0,\n",
    "                     \"Black or African-American\": 0,\n",
    "                     \"Hispanic or Latino\": 0,\n",
    "                     \"White\": 0}) \\\n",
    "                    .withColumnRenamed(\"American Indian and Alaska Native\", \"american_indian_alaska_native\") \\\n",
    "                    .withColumnRenamed(\"Asian\",\"asian\") \\\n",
    "                    .withColumnRenamed(\"Black or African-American\",\"african_american\") \\\n",
    "                    .withColumnRenamed(\"Hispanic or Latino\",\"hispanic_latino\") \\\n",
    "                    .withColumnRenamed(\"White\",\"white\")\n",
    "    \n",
    "    # write data to local as parquet files\n",
    "    # remove directory if already exist\n",
    "    if os.path.exists(output_data + 'demography'):\n",
    "        os.rmdir(output_data + 'demography')\n",
    "    else:\n",
    "        df_demo.write.partitionBy('state','city').parquet('demography')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_demo_read_from = 'us-cities-demographics.csv'\n",
    "df_demo_save_to   = 'file:/home/workspace/'\n",
    "delimiter         = ';'\n",
    "clean_df_demography(delimiter,df_demo_read_from,df_demo_save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- median_age: float (nullable = true)\n",
      " |-- male_population: integer (nullable = true)\n",
      " |-- female_population: integer (nullable = true)\n",
      " |-- total_population: integer (nullable = true)\n",
      " |-- veterans_num: integer (nullable = true)\n",
      " |-- foreign_born_population: integer (nullable = true)\n",
      " |-- avg_household_size: float (nullable = true)\n",
      " |-- state_code: string (nullable = true)\n",
      " |-- american_indian_alaska_native: integer (nullable = true)\n",
      " |-- asian: integer (nullable = true)\n",
      " |-- african_american: integer (nullable = true)\n",
      " |-- hispanic_latino: integer (nullable = true)\n",
      " |-- white: integer (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+----------+---------------+-----------------+----------------+------------+-----------------------+------------------+----------+-----------------------------+-----+----------------+---------------+------+----------+---------+\n",
      "|median_age|male_population|female_population|total_population|veterans_num|foreign_born_population|avg_household_size|state_code|american_indian_alaska_native|asian|african_american|hispanic_latino| white|     state|     city|\n",
      "+----------+---------------+-----------------+----------------+------------+-----------------------+------------------+----------+-----------------------------+-----+----------------+---------------+------+----------+---------+\n",
      "|      31.4|          54333|            59777|          114110|        5327|                   8371|              2.39|        MI|                         5254| 4168|           34196|          14245| 75688|  Michigan|  Lansing|\n",
      "|      34.0|          45147|            48553|           93700|        4705|                  15717|              2.74|        WA|                         2042| 2248|             924|          44172| 79286|Washington|   Yakima|\n",
      "|      37.6|          37042|            38146|           75188|        5667|                   3050|              2.53|        CO|                         1029| 1549|             950|          13567| 71538|  Colorado| Loveland|\n",
      "|      25.5|          37089|            37138|           74227|        2047|                   9202|              2.36|        IA|                          610| 5533|            7219|           4072| 59839|      Iowa|Iowa City|\n",
      "|      32.3|         117189|           119428|          236617|        6207|                  86756|              2.78|        TX|                         2960|49421|           30356|          97862|132779|     Texas|   Irving|\n",
      "+----------+---------------+-----------------+----------------+------------+-----------------------+------------------+----------+-----------------------------+-----+----------------+---------------+------+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in cleaned df_demo, check schema and first 5 rows\n",
    "df_demo_clear = spark.read.parquet('demography')\n",
    "df_demo_clear.printSchema()\n",
    "df_demo_clear.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.3 Airport Code Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.3.1 Data Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load airport-codes_csv.csv\n",
    "df_airport = spark.read.csv(\"airport-codes_csv.csv\",header=True,sep=\",\")\n",
    "df_airport.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "244"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count distinct countries\n",
    "df_airport.select('iso_country').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|ident|         type|                name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|         coordinates|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "|  00A|     heliport|   Total Rf Heliport|          11|       NA|         US|     US-PA|    Bensalem|     00A|     null|       00A|-74.9336013793945...|\n",
      "| 00AA|small_airport|Aero B Ranch Airport|        3435|       NA|         US|     US-KS|       Leoti|    00AA|     null|      00AA|-101.473911, 38.7...|\n",
      "| 00AK|small_airport|        Lowell Field|         450|       NA|         US|     US-AK|Anchor Point|    00AK|     null|      00AK|-151.695999146, 5...|\n",
      "| 00AL|small_airport|        Epps Airpark|         820|       NA|         US|     US-AL|     Harvest|    00AL|     null|      00AL|-86.7703018188476...|\n",
      "| 00AR|       closed|Newport Hospital ...|         237|       NA|         US|     US-AR|     Newport|    null|     null|      null| -91.254898, 35.6087|\n",
      "+-----+-------------+--------------------+------------+---------+-----------+----------+------------+--------+---------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select rows for US\n",
    "df_airport.filter(df_airport.iso_country == \"US\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55075"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count rows\n",
    "df_airport.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22757"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count rows where country = US\n",
    "#df_airport.select('iso_country' == 'US').distinct().count()\n",
    "df_airport.select('iso_country').where(\"iso_country = 'US'\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2015"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count distince iata_code for US\n",
    "df_airport.select('iata_code').filter(df_airport.iso_country == \"US\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20738"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count Null iata_code for US\n",
    "df_airport.where(col(\"iata_code\").isNull()).filter(df_airport.iso_country == \"US\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+----------+------------------------------+\n",
      "|iata_code|iso_country|iso_region|municipality                  |\n",
      "+---------+-----------+----------+------------------------------+\n",
      "|ZZV      |US         |US-OH     |Zanesville                    |\n",
      "|ZPH      |US         |US-FL     |Zephyrhills                   |\n",
      "|ZNC      |US         |US-AK     |Nyac                          |\n",
      "|YUM      |US         |US-AZ     |Yuma                          |\n",
      "|YNG      |US         |US-OH     |Youngstown/Warren             |\n",
      "|YKN      |US         |US-SD     |Yankton                       |\n",
      "|YKM      |US         |US-WA     |Yakima                        |\n",
      "|YIP      |US         |US-MI     |Detroit                       |\n",
      "|YAK      |US         |US-AK     |Yakutat                       |\n",
      "|XSD      |US         |US-NV     |Tonopah                       |\n",
      "|XPR      |US         |US-SD     |Pine Ridge                    |\n",
      "|XNA      |US         |US-AR     |Fayetteville/Springdale/Rogers|\n",
      "|XMD      |US         |US-SD     |Madison                       |\n",
      "|XES      |US         |US-WI     |Lake Geneva                   |\n",
      "|WYS      |US         |US-MT     |West Yellowstone              |\n",
      "|WYB      |US         |US-AK     |Yes Bay                       |\n",
      "|WWT      |US         |US-AK     |Newtok                        |\n",
      "|WWR      |US         |US-OK     |Woodward                      |\n",
      "|WWP      |US         |US-AK     |Whale Pass                    |\n",
      "|WWD      |US         |US-NJ     |Wildwood                      |\n",
      "+---------+-----------+----------+------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check \"code\" and city columns to find out the relationship\n",
    "df_airport.select(col('iata_code'),col('iso_country'),col('iso_region'),col('municipality')) \\\n",
    "            .distinct().sort(df_airport.iata_code.desc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+---------+--------+\n",
      "|ident|type|name|elevation_ft|continent|iso_country|iso_region|municipality|gps_code|iata_code|local_code|longitude|latitude|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+---------+--------+\n",
      "|    0|   0|   0|          34|        0|          0|         0|           6|      81|        0|        50|        0|       0|\n",
      "+-----+----+----+------------+---------+-----------+----------+------------+--------+---------+----------+---------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Get count of both null and missing values\n",
    "df_airport.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_airport.columns]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---------+----------+\n",
      "|ident|gps_code|iata_code|local_code|\n",
      "+-----+--------+---------+----------+\n",
      "| KZZV|    KZZV|      ZZV|       ZZV|\n",
      "| KZPH|    KZPH|      ZPH|       ZPH|\n",
      "|  ZNC|     ZNC|      ZNC|       ZNC|\n",
      "| KNYL|    KNYL|      YUM|       NYL|\n",
      "| KYNG|    KYNG|      YNG|       YNG|\n",
      "| KYKN|    KYKN|      YKN|       YKN|\n",
      "| KYKM|    KYKM|      YKM|       YKM|\n",
      "| KYIP|    KYIP|      YIP|       YIP|\n",
      "| PAYA|    PAYA|      YAK|       YAK|\n",
      "| KTNX|    KTNX|      XSD|       TNX|\n",
      "+-----+--------+---------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check id, gps_code, iata_code, local_code and explor the relationship between those codes\n",
    "df_airport.select('ident','gps_code','iata_code','local_code').orderBy(col('iata_code').desc()).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.3.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_df_airport(delimiter,input_data,output_data):\n",
    "    \"\"\"\n",
    "    cleaning df_airport data\n",
    "        delimiter  : delimiter of csv file\n",
    "        input_data : location of the source data\n",
    "        output_data: location where to save the clean data   \n",
    "    \"\"\"\n",
    "    \n",
    "    # load data\n",
    "    df_airport = spark.read.csv(input_data,header=True,sep=delimiter)\n",
    "\n",
    "    # delete rows with iata_code as Null, None or empty string where country == US\n",
    "    df_airport = df_airport.where(\"iso_country = 'US'\") \\\n",
    "                        .filter(col('iata_code').isNotNull() | \n",
    "                        ~col('iata_code').contains('None') | \\\n",
    "                        ~col('iata_code').contains('NULL') | \\\n",
    "                            (col('iata_code') != '' ))\n",
    "    \n",
    "    # split \"coordinates\" into seperate columns\n",
    "    split_col = split(df_airport['coordinates'], ',')\n",
    "    df_airport = df_airport.withColumn('longitude', split_col.getItem(0)) \\\n",
    "                        .withColumn('latitude', split_col.getItem(1)) \\\n",
    "                        .drop('coordinates','continent','iso_country')\n",
    "    \n",
    "    # write the end data to output location as parquet files\n",
    "    # remove directory if already exist\n",
    "    if os.path.exists(output_data + 'airport'):\n",
    "        os.rmdir(output_data + 'airport')\n",
    "    else:\n",
    "    # write to parquet\n",
    "        df_airport.write.partitionBy('iso_region').parquet('airport')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_airport_delimiter = \",\"\n",
    "df_airport_read_from = \"airport-codes_csv.csv\"\n",
    "df_airport_save_to   = \"file:/home/workspace/\"\n",
    "clean_df_airport(df_airport_delimiter,df_airport_read_from,df_airport_save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ident: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- elevation_ft: string (nullable = true)\n",
      " |-- municipality: string (nullable = true)\n",
      " |-- gps_code: string (nullable = true)\n",
      " |-- iata_code: string (nullable = true)\n",
      " |-- local_code: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- iso_region: string (nullable = true)\n",
      "\n",
      "+-----+-------------+--------------------+------------+-------------+--------+---------+----------+--------------+--------------+----------+\n",
      "|ident|         type|                name|elevation_ft| municipality|gps_code|iata_code|local_code|     longitude|      latitude|iso_region|\n",
      "+-----+-------------+--------------------+------------+-------------+--------+---------+----------+--------------+--------------+----------+\n",
      "|  0AK|small_airport|Pilot Station Air...|         305|Pilot Station|    null|      PQS|       0AK|   -162.899994|     61.934601|     US-AK|\n",
      "|  13Z|seaplane_base|Loring Seaplane Base|           0|       Loring|     13Z|      WLR|       13Z|-131.636993408| 55.6012992859|     US-AK|\n",
      "|  16A|small_airport| Nunapitchuk Airport|          12|  Nunapitchuk|    PPIT|      NUP|       16A|   -162.440454|     60.905591|     US-AK|\n",
      "|  16K|seaplane_base|Port Alice Seapla...|           0|   Port Alice|     16K|      PTC|       16K|      -133.597|        55.803|     US-AK|\n",
      "| 19AK|small_airport|     Icy Bay Airport|          50|      Icy Bay|    19AK|      ICY|      19AK|-141.662002563|   59.96900177|     US-AK|\n",
      "+-----+-------------+--------------------+------------+-------------+--------+---------+----------+--------------+--------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in cleaned df_airport, check schema and first 5 rows\n",
    "df_airport_clear = spark.read.parquet('airport')\n",
    "df_airport_clear.printSchema()\n",
    "df_airport_clear.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 2.4 World Temperature Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.4.1 Data Exploring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|        dt|AverageTemperature|AverageTemperatureUncertainty| City|Country|Latitude|Longitude|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "|1743-11-01|             6.068|           1.7369999999999999|rhus|Denmark|  57.05N|   10.33E|\n",
      "|1743-12-01|              null|                         null|rhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-01-01|              null|                         null|rhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-02-01|              null|                         null|rhus|Denmark|  57.05N|   10.33E|\n",
      "|1744-03-01|              null|                         null|rhus|Denmark|  57.05N|   10.33E|\n",
      "+----------+------------------+-----------------------------+-----+-------+--------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load world temperature data\n",
    "#fname = '../../data2/GlobalLandTemperaturesByCity.csv'\n",
    "df_temperature = spark.read.csv(\"../../data2/GlobalLandTemperaturesByCity.csv\",header=True,sep=\",\")\n",
    "df_temperature.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- City: string (nullable = true)\n",
      " |-- Country: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_temperature.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+--------+---------+\n",
      "|City|Country|Latitude|Longitude|\n",
      "+----+-------+--------+---------+\n",
      "|   0|      0|       0|        0|\n",
      "+----+-------+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Get count of both null and missing values\n",
    "df_temperature.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_temperature.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 2.4.2 Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def clean_df_temperature(delimiter,input_data,output_data):\n",
    "    \"\"\"\n",
    "    cleaning df_airport data\n",
    "        delimiter  : delimiter of csv file\n",
    "        input_data : location of the source data\n",
    "        output_data: location where to save the clean data   \n",
    "    \"\"\"\n",
    "    # load data\n",
    "    df_temperature = spark.read.csv(input_data,header=True,sep=delimiter) \n",
    "    \n",
    "    # slice table with interested columns (City, Country, Latitude and Longitude) only\n",
    "    df_temperature = df_temperature.select('City','Country','Latitude','Longitude') \\\n",
    "                                .filter(df_temperature.Country == 'United States') \\\n",
    "                                .distinct()\n",
    "    \n",
    "    # drop duplicated rows and change column names\n",
    "    df_temperature = df_temperature.dropDuplicates() \\\n",
    "                                .withColumnRenamed(\"City\",\"city\") \\\n",
    "                                .withColumnRenamed(\"Latitude\",\"latitude\") \\\n",
    "                                .withColumnRenamed(\"Longitude\",\"longitude\") \\\n",
    "                                .drop(\"Country\")\n",
    "    \n",
    "    # write the end data to output location as parquet files\n",
    "    # remove directory if already exist\n",
    "    if os.path.exists(output_data + 'coordinates'):\n",
    "        os.rmdir(output_data + 'coordinates')\n",
    "    else:\n",
    "    # write to parquet\n",
    "        df_temperature.write.partitionBy('city').parquet('coordinates')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "df_temperature_delimiter = \",\"\n",
    "df_temperature_read_from = \"../../data2/GlobalLandTemperaturesByCity.csv\"\n",
    "df_temperature_save_to   = \"file:/home/workspace/\"\n",
    "clean_df_temperature(df_temperature_delimiter,df_temperature_read_from,df_temperature_save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- latitude: string (nullable = true)\n",
      " |-- longitude: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      "\n",
      "+--------+---------+-------------+\n",
      "|latitude|longitude|         city|\n",
      "+--------+---------+-------------+\n",
      "|  37.78N|  103.73W|       Pueblo|\n",
      "|  34.56N|  118.70W|Thousand Oaks|\n",
      "|  34.56N|  118.70W|     El Monte|\n",
      "|  37.78N|  122.03W|      Concord|\n",
      "|  37.78N|  122.03W|  Santa Clara|\n",
      "+--------+---------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# read in cleaned df_temperature, check schema and first 5 rows\n",
    "df_coordinates_clear = spark.read.parquet('coordinates')\n",
    "df_coordinates_clear.printSchema()\n",
    "df_coordinates_clear.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model (see README)\n",
    "I decided on a Star Schema for my database, the fact and dimension tables look like the following:       \n",
    "\n",
    "#### Fact table\n",
    "\n",
    "__fact_immigration_record__:        \n",
    "*__cic_id (PK)__, port(FK), arrival_date, arrive_year, arrive_month, departure_date, ariline, flight_num, arrive_city (FK), arrive_state (FK), mode*\n",
    "\n",
    "#### Dimension Tables   \n",
    "1. __dim_immigrant__: *__cic_id (PK)__, age, occupation, gender, birth_year, citizen_country,resident_country*\n",
    "\n",
    "2. __dim_city__: *city, state, state_code, longitude, latitude, median_age, avg_household_size, total_population,\n",
    "male_population, female_population, veterans_num, foreign_born_population, american_indian_alaska_native, asian,african_american, hispanic_latino, white, __(city,state PK)__*\n",
    "\n",
    "3. __dim_airport__: *__id (PK)__, type, name, elevation_ft, iso_region, municipality, gps_code, iata_code (FK) reference fact_port, local_code, longitude, latitude*\n",
    "\n",
    "4. __dim_visa__: *__cic_id (PK)__, visa_type, visa_class, visa_issue_state, rrive_flag, departure_flag, update_flag, match_flag, allowed_date*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_data_model():\n",
    "    \"\"\"\n",
    "    ETL process to create data model including one fact table and four dimension tables and write them to paquet files    \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    # create fact table    \n",
    "    df_airport_temp = df_airport_clear.select('iata_code','municipality').filter(df_airport_clear.municipality.isNotNull())\n",
    "    df_sas_temp = df_sas_clear.select(\"cic_id\",\"port\",\"mode\",\"arrival_date\",\"arrive_year\",\"arrive_month\", \\\n",
    "                                        \"departure_date\",\"airline\",\"flight_num\",\"arrive_state\").distinct()\n",
    "    cond = [df_sas_temp['port'] == df_airport_temp['iata_code']]\n",
    "    fact_immigration_record = df_sas_temp.join(df_airport_temp, cond,'left').drop('iata_code') \\\n",
    "                                    .select(\"cic_id\",\"port\",\"mode\",\"arrival_date\",\"arrive_year\",\"arrive_month\", \\\n",
    "                                        \"departure_date\",\"airline\",\"flight_num\",\"arrive_state\",\"municipality\") \\\n",
    "                                    .withColumnRenamed('municipality','arrive_city') \\\n",
    "                                    .distinct()\n",
    "            \n",
    "    # Create dim_immigrant\n",
    "    dim_immigrant = df_sas_clear.select(\"cic_id\",\"age\",\"birth_year\",\"gender\",\"occupation\", \\\n",
    "                              \"citizen_country\",\"resident_country\",\"arrive_state\").distinct()\n",
    "       \n",
    "    # Create dim_city\n",
    "    city_list = set(fact_immigration_record.select('arrive_city').toPandas()['arrive_city'])\n",
    "    df_coordinates_temp = df_coordinates_clear.selectExpr('city as temp_city','latitude','longitude')\n",
    "    dim_city = df_demo_clear.join(df_coordinates_temp,df_demo_clear.city == df_coordinates_temp.temp_city, 'left') \\\n",
    "                    .select('city','state','state_code','latitude','longitude', \\\n",
    "                           'median_age','male_population','female_population', \\\n",
    "                           'total_population','veterans_num','foreign_born_population', \\\n",
    "                           'avg_household_size','american_indian_alaska_native', \\\n",
    "                           'asian','african_american','hispanic_latino','white') \\\n",
    "                    .filter(df_demo_clear.city.isin(city_list)).distinct()   \n",
    "    \n",
    "    # create dim_airport\n",
    "    # select distinct port codes from df_sas\n",
    "    port_list = set(df_sas_clear.select('port').filter(df_sas_clear.port.isNotNull()).toPandas()['port'])\n",
    "    \n",
    "    # load dim_airport reserving only the airport included in fact table\n",
    "    dim_airport = df_airport_clear.select('ident', 'type', 'name', 'elevation_ft', \\\n",
    "                                'municipality', 'gps_code', 'iata_code',\\\n",
    "                                'local_code', 'longitude', 'latitude', 'iso_region') \\\n",
    "                        .filter(df_airport_clear.iata_code.isin(port_list)) \\\n",
    "                        .filter(df_airport_clear.municipality.isNotNull()) \\\n",
    "                        .distinct()    \n",
    "    \n",
    "    # create dim_visa\n",
    "    dim_visa = df_sas_clear.select('cic_id','visa_type','visa_class','visa_issue_state','arrive_flag', \\\n",
    "                         'departure_flag','update_flag','match_flag','allowed_date') \\\n",
    "                    .distinct()\n",
    "    \n",
    "    # write to parquet\n",
    "    fact_immigration_record.write.partitionBy(\"arrive_state\").parquet(\"US_immigration\")\n",
    "    dim_immigrant.write.partitionBy(\"arrive_state\").parquet(\"US_immigrant\")\n",
    "    dim_city.write.partitionBy(\"state_code\").parquet(\"US_city\")\n",
    "    dim_airport.write.partitionBy(\"iso_region\").parquet(\"US_airport\")\n",
    "    dim_visa.write.partitionBy(\"visa_type\").parquet(\"US_visa\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "create_data_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Quality checks are performed to ensure the pipeline ran as expected. These included:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# load final tables \n",
    "fact = spark.read.parquet(\"US_immigration\")\n",
    "dim_immigrant = spark.read.parquet(\"US_immigrant\")\n",
    "dim_city = spark.read.parquet(\"US_city\")\n",
    "dim_airport = spark.read.parquet(\"US_airport\")\n",
    "dim_visa = spark.read.parquet(\"US_visa\")\n",
    "tablelist = [fact,dim_immigrant,dim_city,dim_airport,dim_visa]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# check rows in tables and column types\n",
    "def check_rows(tablelist):\n",
    "    for table in tablelist:\n",
    "        if table.count() > 0:\n",
    "            print(\"{} table is not empty\".format(table))\n",
    "        else:\n",
    "            print(\"No records in {} table\".format(table))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[cic_id: int, port: string, mode: int, arrival_date: date, arrive_year: int, arrive_month: int, departure_date: date, airline: string, flight_num: string, arrive_city: string, arrive_state: string] table is not empty\n",
      "DataFrame[cic_id: int, age: int, birth_year: int, gender: string, occupation: string, citizen_country: int, resident_country: int, arrive_state: string] table is not empty\n",
      "DataFrame[city: string, state: string, latitude: string, longitude: string, median_age: float, male_population: int, female_population: int, total_population: int, veterans_num: int, foreign_born_population: int, avg_household_size: float, american_indian_alaska_native: int, asian: int, african_american: int, hispanic_latino: int, white: int, state_code: string] table is not empty\n",
      "DataFrame[ident: string, type: string, name: string, elevation_ft: string, municipality: string, gps_code: string, iata_code: string, local_code: string, longitude: string, latitude: string, iso_region: string] table is not empty\n",
      "DataFrame[cic_id: int, visa_class: int, visa_issue_state: string, arrive_flag: string, departure_flag: string, update_flag: string, match_flag: string, allowed_date: date, visa_type: string] table is not empty\n"
     ]
    }
   ],
   "source": [
    "check_rows(tablelist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def check_unique_keys(fact,dim1,dim2,dim3,dim4):\n",
    "    if fact.groupBy(\"cic_id\").count().filter(\"count > 1\").count() == 0:\n",
    "        print(\"Fact table's keys are unique\")\n",
    "    else:\n",
    "        print(\"Fact table's keys are not unique\")\n",
    "    if dim1.groupBy(\"cic_id\").count().filter(\"count > 1\").count() == 0:\n",
    "        print(\"Dimention table's keys are unique\")\n",
    "    else:\n",
    "        print(\"Dimention table's keys are not unique\")\n",
    "    if dim2.groupBy(\"city\",\"state\").count().filter(\"count > 1\").count() == 0:\n",
    "        print(\"Dimention table's keys are unique\")\n",
    "    else:\n",
    "        print(\"Dimention table's keys are not unique\")\n",
    "    if dim3.groupBy(\"iata_code\").count().filter(\"count > 1\").count() == 0:\n",
    "        print(\"Dimention table's keys are unique\")\n",
    "    else:\n",
    "        print(\"Dimention table's keys are not unique\")\n",
    "    if dim4.groupBy(\"cic_id\").count().filter(\"count > 1\").count() == 0:\n",
    "        print(\"Dimention table's keys are unique\")\n",
    "    else:\n",
    "        print(\"Dimention table's keys are not unique\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fact table's keys are unique\n",
      "Dimention table's keys are unique\n",
      "Dimention table's keys are unique\n",
      "Dimention table's keys are unique\n",
      "Dimention table's keys are unique\n"
     ]
    }
   ],
   "source": [
    "check_unique_keys(*tablelist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "Data dictionary is drafted in a separate file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.          \n",
    "  The source datasets are relativly large (e.g. the immigration data included 3096313 rows), from the efficiency perspective, using PySpark can process the data in parallel and save time. Spark is also easy to deploy on cloud and easy to scale. If the data volume will increase, the complete process can be moved to a cloud hosted Spark cluster, the complete process could also be adopted. A Star Schema is used for data model, because I assumed that we already know the analysis purpose of our data, which is in this project to analysis the immigration data to U.S., in this case, we could create a data model instead of turning to a unstructured data lake architecture. Instead of normalizing the tables or using a Snowflake Schema, I decided for a Star Schema, because of its simplicity for users to write, and databases to process: queries are written with simple inner joins between the facts and a small number of dimensions. Star joins are simpler than possible in snowflake schema.\n",
    "  \n",
    "* Propose how often the data should be updated and why.        \n",
    "  In princip should be updated every time a new immigrant is registered or according to the consumer demand\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    "\n",
    " * The data was increased by 100x.       \n",
    "  As storage, another scalable Cloud-based datawarehouse/data lake or on-premise location would be proper. PySpark or other paralyzed frameworks would still be preferred because Apache Spark is linearly scalable, which means I could simply add the number of clusters to increase the performance. Specifically, I would run Spark on multiple clusters using services like EMR. \n",
    "  \n",
    " * The data populates a dashboard that must be updated daily by 7 am every day.      \n",
    " I would move the database on a Cloud platform (AWS for example) and connect with proper BI tools (Dash, Tableau for example) that are connected to the cloud platform and automate the entire data flow using tools like Airflow. A popular implementation here is a combination of Airflow + Spark + Apache Livy in EMR cluster so that Spark commands can be passed through an API interface.\n",
    " \n",
    " * The database needed to be accessed by 100+ people.     \n",
    " Apache Hive or AWS Redshift will meet the need. \n",
    " Amazon maintains a software fork of Apache Hive included in Amazon Elastic MapReduce on Amazon Web Services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
